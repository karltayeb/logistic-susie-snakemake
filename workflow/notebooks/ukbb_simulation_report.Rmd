---
title: "UKBB Simulation Report"
description: "We compare the performance of logistic-IBSS to linear SuSiE, RSS with logistic regression summary stats, and others on a set of simulations meant to emulate realistic case-control data in GWAS. We aim to identify regimes where the logistic regression model is necessary."
output: 
    html_document:
        toc: true
        toc_float: true
        toc_depth: 3
        code_folding: hide 
---

```{r setup}
#project_dir <- snakemake@config$project_dir
project_dir <-  '/project2/mstephens/ktayeb/logistic-susie-snakemake'
knitr::opts_knit$set(root.dir = project_dir)
```

```{r setup2}
library(dplyr)
library(dplyr)
library(reticulate)
library(tidyr)
library(tictoc)
library(ggplot2)

source('workflow/scripts/plotting/plot_functions.R')

# httpgd::hgd()
res_pip <- readRDS("results/ukbb_geno/chr1_100794065_101983457/pip_summary.rds")
res_cs <- readRDS("results/ukbb_geno/chr1_100794065_101983457/cs_summary.rds")
manifest <- readr::read_delim('config/ukbb_sim/ukbb_sim_manifest.tsv', delim='\t')
```

```{r plot-funs}
threshold_var <- function(df, var, val, comp){
#   var <- as_string(ensym(var))
  filter(df, comp(.data[[var]], !!val))
}

cs_filtered_coverage <- function(res_cs, var, val, comp){
    Q_tbl <- res_cs %>%
        select(c(method, sim_id, q)) %>%
        unique() %>%
        group_by(method) %>%
        summarize(Q = sum(q))

    res_cs %>%
        left_join(Q_tbl) %>%
        threshold_var(var, val, comp) %>%
        mutate(val = val) %>%
        group_by(method) %>%
        summarize(coverage = mean(covered), n_found2 = list(table(n_found)), mean_cs_size = mean(cs_size), val = val[1], Q=Q[1], n_found = sum(n_found)) %>%
        unnest_wider(n_found2) 
}

make_plot <- function(res_cs, var, vals, op){
    f <- function(val){
        res_cs %>%
            cs_filtered_coverage(var, val, op)
    }
    filtered_coverage <- purrr::map_dfr(vals, f)

    a <- filtered_coverage %>%
        ggplot(aes(x = val, y = coverage, color = method)) +
        geom_path() + 
        geom_hline(yintercept = 0.95) +
        xlab(var)

    b <- filtered_coverage %>%
        mutate(power = n_found/Q) %>%
        mutate(power = `1`/Q) %>%
        ggplot(aes(x = val, y = power, color = method)) +
        geom_path() +
        xlab(var)

    cowplot::plot_grid(a, b)
}

```
## SER simulations

### Simulation settings

```{r}
# show unique simulation settings for SER simulations
manifest %>%
    filter(name == 'ser') %>%
    select(c(q, k, h2, rho, gamma)) %>%
    unique() %>%
    rmarkdown::paged_table()
```

### Credible sets

#### CS size distribution 
```{r}
# histogram of CSs, and coverage over all simulations
res_cs %>%
    filter(name == 'ser') %>%
    ggplot(aes(x = cs_size)) + 
    geom_histogram() +
    facet_wrap(vars(method))
```

#### CS coverage

```{r}
res_cs %>%
    filter(name == 'ser') %>%
    group_by(method) %>%
    summarize(coverage = mean(covered), n_found = list(table(n_found)), mean_cs_size = mean(cs_size)) %>%
    unnest_wider(n_found) %>%
    knitr::kable()
```

The Laplace ABF gives lower coverage marginally over all CSs and all simulation scenarios.
This is resolved by filtering out large CSs, which are largely uninformative.

```{r}
res_cs %>%
    filter(name == 'ser') %>%
    filter(cs_size < 200) %>%
    group_by(method) %>%
    summarize(coverage = mean(covered), n_found = list(table(n_found)), mean_cs_size = mean(cs_size)) %>%
    unnest_wider(n_found) %>%
    knitr::kable()
```

#### Conditional coverage

We can take a closer look at coverage across different simulation scenarios.

SER methods (both Laplace and ABF approximation) exhibit poor coverage in the most unbalanced case-control ratio.
Coverage improves for both methods when the case-control ratio is less severe.

If we look at coverage as a function of signal strength (`gamma` = PVE by casual variant on liability),
SER methods slightly undercover for weak effects and very strong effects. 
The mis-coverage of the ABF approximation is much worse than the Laplace approximation for the strongest signals.

```{r}
manifest %>%
    filter(name == 'ser') %>%
    {table(.$k)}

res_cs %>%
    filter(name == 'ser') %>%
    filter(cs_size < 200) %>%
    group_by(method, k) %>%
    summarize(coverage = mean(covered), n_found = list(table(n_found)), mean_cs_size = mean(cs_size)) %>%
    unnest_wider(n_found) %>%
    select(method, k, coverage) %>%
    pivot_wider(names_from = k, values_from = coverage) %>%
    knitr::kable()
```

```{r}
# gamma = PVE of causal variant on liability
manifest %>%
    filter(name == 'ser') %>%
    select(gamma) %>%
    table()

res_cs %>%
    filter(name == 'ser') %>%
    filter(cs_size < 200) %>%
    group_by(method, gamma) %>%
    summarize(coverage = mean(covered), n_found = list(table(n_found)), mean_cs_size = mean(cs_size)) %>%
    unnest_wider(n_found) %>%
    select(method, gamma, coverage) %>%
    pivot_wider(names_from = gamma, values_from = coverage) %>%
    knitr::kable()
```

```{r}
# gamma x rho
manifest %>%
    filter(name == 'ser') %>%
    select(c(gamma, rho)) %>%
    table %>%
    knitr::kable()

res_cs %>%
    filter(name == 'ser') %>%
    filter(cs_size < 200) %>%
    group_by(method, gamma, rho) %>%
    summarize(coverage = mean(covered), n_found = list(table(n_found)), mean_cs_size = mean(cs_size)) %>%
    unnest_wider(n_found) %>%
    select(method, gamma, rho, coverage) %>%
    pivot_wider(names_from = rho, values_from = coverage) %>%
    knitr::kable()
```


```{r}
# filter on purity
vals <- c(seq(0.1, 0.9, by = 0.1), seq(0.91, .99, by = 0.01))
res_cs %>% filter(name == 'ser') %>% make_plot('purity', vals, `>`)

# filter on cs size
vals <- seq(5, 100, by=5)
res_cs %>% filter(name == 'ser') %>% make_plot('cs_size', vals, `<`)

# filter on the SER-level BFs
# note multiple CSs may capture the same effect causing "power" > 1
vals <- seq(0, 100, by=5)
res_cs %>% filter(name == 'ser') %>% make_plot('lbf_ser', vals, `>`)
```

### Power vs FDP (PIP ordering)

ABF and RSS L = 1, expectedly give very similar PIPs, 
since the RSS likelihood for L=1 essentially uses ABFs 
(differences are due to differences in estimating prior variance hyper-parameter).

The Laplace ABF gives better ordering for the PIPs, that is a win!

```{r}
# fdp vs power
fdp_plot <- res_pip %>%
    filter(name == 'ser') %>%
    unnest_longer(c(pip, causal)) %>%
    ggplot(aes(pip, causal, color=method)) + 
    stat_fdp_power(geom='path', max_fdp=1.0) +
    labs(x = 'FDP', y = 'Power')
fdp_plot
```

### PIP Calibration 

PIP calibration looks a little messy-- may need more simulations to get a clear picture.
Note that for the logistic SERs the only source of error here is the approximation error of using the Laplace ABF vs Wakefield ABF.
TODO: add comparison to exact computation of the BF via quadrature.

```{r}
pip_calibration_plot <- res_pip %>%
    filter(name == 'ser') %>%
    unnest_longer(c(pip, causal)) %>%
    ggplot(aes(pip, causal)) +
    stat_pip_calibration() +
    geom_abline(intercept = 0, slope = 1, color='red') +
    theme_bw() +
    labs(x='PIP', y='Frequency') + 
    facet_wrap(vars(method))
pip_calibration_plot
```

## $L \geq 1$ simulations

### Simulation settings

```{r}
# show unique simulation settings for SER simulations
manifest %>%
    filter(name == 'main') %>%
    select(c(q, k, h2, rho, gamma)) %>%
    unique() %>%
    rmarkdown::paged_table() 
```


### Credible Sets

#### CS size distribution 

```{r}
# histogram of CSs, and coverage over all simulations
res_cs %>%
    filter(name == 'main') %>%
    ggplot(aes(x = cs_size)) + 
    geom_histogram() +
    facet_wrap(vars(method), ncol=4)
```

#### CS Coverage

```{r}
res_cs %>%
    filter(name == 'main') %>%
    group_by(method) %>%
    summarize(coverage = mean(covered), n_found = list(table(n_found)), mean_cs_size = mean(cs_size)) %>%
    unnest_wider(n_found) %>%
    knitr::kable()
```


#### Conditional coverage

```{r}
# filter on purity
vals <- c(seq(0.1, 0.9, by = 0.1), seq(0.91, .99, by = 0.01))
res_cs %>% filter(name == 'main') %>% make_plot('purity', vals, `>`)

# filter on cs size
vals <- seq(5, 100, by=5)
res_cs %>% filter(name == 'main') %>% make_plot('cs_size', vals, `<`)

# filter on the SER-level BFs
# note multiple CSs may capture the same effect causing "power" > 1
vals <- seq(0, 100, by=5)
res_cs %>% filter(name == 'main') %>% make_plot('lbf_ser', vals, `>`)


res_cs %>% filter(name == 'main') %>% cs_filtered_coverage('lbf_ser', 20, `>`)
cs_filtered_coverage
```

Here we plot the coverage and power (proportion of true effect captured in CSs) for each method, at varying CS size cutoffs.
We see good coverage for most methods across all CS size cutoffs except for RSS (L=5) and GIBSS-ABF.
Interestingly, we see coverage increases for small CSs for Laplace ABF methods (GIBSS_LABF, SER),
while it tends to decrease for ABF methods (SER_ABF, RSS_L1).

Linear SuSiE has good coverage and makes the most discoveries at each CS size threshold.

#### CS coverage stratified by simulation parameters

ABF SuSiE and RSS have poor coverage compared to the other methods.
We can take a closer look at coverage across different simulation scenarios.

SER methods (both Laplace and ABF approximation) exhibit poor coverage in the most unbalanced case-control ratio.
Coverage improves for both methods when the case-control ratio is less severe.

If we look at coverage conditional on signal strength (`gamma` = PVE by casual variant on liability),
SER methods slightly undercover for weak effects and very strong effects. 
The mis-coverage of the ABF approximation is much worse than the Laplace approximation for the strongest signals.

```{r}
manifest %>%
    filter(name == 'main') %>%
    {table(.$k)} %>%
    knitr::kable()

res_cs %>%
    filter(name == 'main') %>%
    filter(cs_size < 200) %>%
    group_by(method, k) %>%
    summarize(coverage = mean(covered), n_found = list(table(n_found)), mean_cs_size = mean(cs_size)) %>%
    unnest_wider(n_found) %>%
    select(method, k, coverage) %>%
    pivot_wider(names_from = k, values_from = coverage) %>%
    knitr::kable()


# gamma = PVE of causal variant on liability
manifest %>%
    filter(name == 'main') %>%
    {table(.$gamma)} %>%
    knitr::kable()

res_cs %>%
    filter(name == 'main') %>%
    filter(cs_size < 200) %>%
    group_by(method, gamma) %>%
    summarize(coverage = mean(covered), n_found = list(table(n_found)), mean_cs_size = mean(cs_size)) %>%
    unnest_wider(n_found) %>%
    select(method, gamma, coverage) %>%
    pivot_wider(names_from = gamma, values_from = coverage) %>%
    knitr::kable()

# gamma x rho
manifest %>%
    filter(name == 'main') %>%
    select(c(gamma, rho)) %>%
    table
```



#### FDP vs Power

Laplace logistic IBSS, and linear SuSiE perform best, followed by the Laplace logistic SER.
RSS performs very poorly.

```{r}
# fdp vs power
fdp_plot <- res_pip %>%
    filter(name == 'main') %>%
    unnest_longer(c(pip, causal)) %>%
    ggplot(aes(pip, causal, color=method)) + 
    stat_fdp_power(geom='path', max_fdp=1.0) +
    labs(x = 'FDP', y = 'Power')
fdp_plot
```

#### PIP Calibrartion

PIP calibration looks a little messy-- may need more simulations to get a clear picture.
Note that for the logistic SERs the only source of error here is the approximation error of using the Laplace ABF vs Wakefield ABF.
TODO: add comparison to exact computation of the BF via quadrature.

```{r}
pip_calibration_plot <- res_pip %>%
    filter(name == 'main') %>%
    unnest_longer(c(pip, causal)) %>%
    ggplot(aes(pip, causal)) +
    stat_pip_calibration() +
    geom_abline(intercept = 0, slope = 1, color='red') +
    theme_bw() +
    labs(x='PIP', y='Frequency') + 
    facet_wrap(vars(method))
pip_calibration_plot
```

